{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import numpy as np\n",
    "from hwr_utils import *\n",
    "from hwr_utils.stroke_plotting import *\n",
    "from hwr_utils.stroke_recovery import *\n",
    "import json\n",
    "from gen_preds_offline import load_all_gts\n",
    "\n",
    "from hwr_utils import visualize\n",
    "from torch.utils.data import DataLoader\n",
    "from loss_module.stroke_recovery_loss import StrokeLoss\n",
    "from trainers import TrainerStrokeRecovery\n",
    "from hwr_utils.stroke_dataset import BasicDataset\n",
    "from hwr_utils.stroke_recovery import *\n",
    "from hwr_utils import utils\n",
    "from torch.optim import lr_scheduler\n",
    "from models.stroke_model import StrokeRecoveryModel\n",
    "from train_stroke_recovery import parse_args, graph\n",
    "from hwr_utils.hwr_logger import logger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(pred,gt):\n",
    "    return move_bad_points(reference=gt, moving_component=pred, reference_is_image=True)\n",
    "\n",
    "def eval_only(dataloader, model):\n",
    "    final_out = []\n",
    "    for i, item in enumerate(dataloader):\n",
    "        preds = TrainerStrokeRecovery.eval(item[\"line_imgs\"], model,\n",
    "                                           label_lengths=item[\"label_lengths\"],\n",
    "                                           relative_indices=config.pred_relativefy,\n",
    "                                           sigmoid_activations=config.sigmoid_indices)\n",
    "\n",
    "        # Pred comes out of eval WIDTH x VOCAB\n",
    "        preds_to_graph = [post_process(p, item[\"line_imgs\"][i]).permute([1, 0]) for i,p in enumerate(preds)]\n",
    "\n",
    "        # Get GTs, save to file\n",
    "        if i<10:\n",
    "            # Save a sample\n",
    "            save_folder = graph(item, preds=preds_to_graph, _type=\"eval\", epoch=\"current\", config=config)\n",
    "            output_path = (save_folder / \"data\")\n",
    "            output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        names = [Path(p).stem.lower() for p in item[\"paths\"]]\n",
    "        output = []\n",
    "        for ii, name in enumerate(names):\n",
    "            if name in GT_DATA:\n",
    "                output.append({\"stroke\": preds[ii].detach().numpy(), \"text\":GT_DATA[name]})\n",
    "            else:\n",
    "                print(f\"{name} not found\")\n",
    "        utils.pickle_it(output, output_path / f\"{i}.pickle\")\n",
    "        np.save(output_path / f\"{i}.npy\", output)\n",
    "        final_out += output\n",
    "    utils.pickle_it(final_out, output_path / f\"all_data.pickle\")\n",
    "    np.save(output_path / f\"all_data.npy\", final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/data/GitHub/simple_hwr/RESULTS/pretrained/brodie_123/stroke_number_with_BCE_RESUME2.yaml\n",
      "Experiment: new_experiment02, Results Directory: /home/taylor/github/simple_hwr/RESULTS/ver4/20200229_223630-stroke_number_with_BCE_RESUME/new_experiment02\n",
      "Effective logging level: 20\n",
      "Using config file /media/data/GitHub/simple_hwr/RESULTS/pretrained/brodie_123/stroke_number_with_BCE_RESUME2.yaml\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 |   0% |  8% |\n",
      "|  1 | nan% | 46% |\n",
      "Creating LSTM: in:1024 hidden:128 dropout:0.5 layers:2 out:4\n",
      "13:09:35 INFO COORD CONV: y_rel\n",
      "13:09:35 INFO COORD CONV: y_rel\n",
      "13:09:35 INFO COORD CONV: y_rel\n",
      "13:09:35 INFO Zero center False\n",
      "13:09:35 INFO Zero center False\n",
      "13:09:35 INFO Zero center False\n",
      "13:09:35 INFO RECT X Coord: False\n",
      "13:09:35 INFO RECT X Coord: False\n",
      "13:09:35 INFO RECT X Coord: False\n",
      "13:09:35 INFO Normalized X Coord: True\n",
      "13:09:35 INFO Normalized X Coord: True\n",
      "13:09:35 INFO Normalized X Coord: True\n",
      "13:09:35 INFO Using ABS+REL X Coord Channels: False\n",
      "13:09:35 INFO Using ABS+REL X Coord Channels: False\n",
      "13:09:35 INFO Using ABS+REL X Coord Channels: False\n",
      "13:09:35 INFO X: False\n",
      "13:09:35 INFO X: False\n",
      "13:09:35 INFO X: False\n",
      "13:09:35 INFO Y: False\n",
      "13:09:35 INFO Y: False\n",
      "13:09:35 INFO Y: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../hwr_utils/utils.py:33: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return fix_scientific_notation(yaml.load(config.open(mode=\"r\")))\n",
      "../../hwr_utils/utils.py:545: UserWarning: cross_entropy loss already added to stats\n",
      "  warnings.warn(f\"{loss['name']} loss already added to stats\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:09:35 INFO ('Current dataset: ', PosixPath('data/prepare_IAM_Lines/lines'))\n",
      "13:09:35 INFO ('Current dataset: ', PosixPath('data/prepare_IAM_Lines/lines'))\n",
      "13:09:35 INFO ('Current dataset: ', PosixPath('data/prepare_IAM_Lines/lines'))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c6111d90538f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Dataset - just expecting a folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBasicDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m eval_loader=DataLoader(eval_dataset,\n\u001b[0m\u001b[1;32m     32\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwr5/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwr5/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m     94\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "config_path = \"/media/data/GitHub/simple_hwr/RESULTS/pretrained/brodie_123/stroke_number_with_BCE_RESUME2.yaml\"\n",
    "load_path_override = \"/media/data/GitHub/simple_hwr/RESULTS/pretrained/brodie_123/stroke_number_with_BCE_RESUME2_model_123_epochs.pt\"\n",
    "\n",
    "# Make these the same as whereever the file is being loaded from; make the log_dir and results dir be a subset\n",
    "# main_model_path, log_dir, full_specs, results_dir, load_path\n",
    "config = utils.load_config(config_path, hwr=False)\n",
    "\n",
    "# Free GPU memory if necessary\n",
    "if config.device == \"cuda\":\n",
    "    utils.kill_gpu_hogs()\n",
    "\n",
    "batch_size = config.batch_size\n",
    "vocab_size = config.vocab_size\n",
    "device=torch.device(config.device)\n",
    "\n",
    "output = Path(config.results_dir)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "folder = Path(config.dataset_folder)\n",
    "\n",
    "# OVERLOAD\n",
    "folder = Path(\"data/prepare_IAM_Lines/lines/\")\n",
    "gt_path = Path(\"./data/prepare_IAM_Lines/gts/lines/txt\")\n",
    "\n",
    "model = StrokeRecoveryModel(vocab_size=vocab_size, device=device, cnn_type=config.cnn_type, first_conv_op=config.coordconv, first_conv_opts=config.coordconv_opts).to(device)\n",
    "\n",
    "## Loader\n",
    "logger.info((\"Current dataset: \", folder))\n",
    "\n",
    "# Dataset - just expecting a folder\n",
    "eval_dataset=BasicDataset(root=folder, cnn=model.cnn)\n",
    "eval_loader=DataLoader(eval_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=6,\n",
    "                              collate_fn=eval_dataset.collate, # this should be set to collate_stroke_eval\n",
    "                              pin_memory=False)\n",
    "\n",
    "config.n_train_instances = None\n",
    "config.n_test_instances = len(eval_loader.dataset)\n",
    "config.n_test_points = None\n",
    "\n",
    "## Stats\n",
    "if config.use_visdom:\n",
    "    visualize.initialize_visdom(config[\"full_specs\"], config)\n",
    "utils.stat_prep_strokes(config)\n",
    "\n",
    "# Create loss object\n",
    "config.loss_obj = StrokeLoss(loss_names=config.loss_fns, loss_stats=config.stats, counter=config.counter)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0005 * batch_size/32)\n",
    "config.scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=.95)\n",
    "trainer = TrainerStrokeRecovery(model, optimizer, config=config, loss_criterion=config.loss_obj)\n",
    "\n",
    "config.model = model\n",
    "config.load_path = load_path_override if (\"load_path_override\" in locals()) else config.load_path\n",
    "\n",
    "config.sigmoid_indices = TrainerStrokeRecovery.get_indices(config.pred_opts, \"sigmoid\")\n",
    "\n",
    "# Load the GTs\n",
    "load_all_gts(gt_path)\n",
    "print(\"Number of images: {}\".format(len(eval_loader.dataset)))\n",
    "print(\"Number of GTs: {}\".format(len(GT_DATA)))\n",
    "\n",
    "## LOAD THE WEIGHTS\n",
    "utils.load_model_strokes(config) # should be load_model_strokes??????\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "eval_only(eval_loader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwr5",
   "language": "python",
   "name": "hwr5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
