### TESTING
TESTING: false # Load as little as possible for fast prototyping
SMALL_TRAINING: 100

# Load model
load_path: # results/seq2seq/20190913_103617-seq2seq_lr_0.0001_bs_16_warp_True_arch_seq2seq/seq2seq_model.pt
#load_path: /media/data/GitHub/simple_hwr/results/baseline/20190625_092305-lr_0.001_bs_16_warp_True_arch_basic_encoder/baseline_model.pt
test_only: false                             # Do not train

# General
experiment: seq2seq
name: seq2seq # unknown_experiment
output_folder: ./results
epochs_to_run: 500
save_freq: 5
use_visdom: true
debug: off
use_gpu: true

# results_dir:
# log_dir:

# Training data
training_jsons: [prepare_IAM_Lines/raw_gts/lines/txt/training.json]
training_root: ../simple_hwr/data
training_shuffle: true
training_warp: true
online_augmentation: true
writer_id_pickles: [prepare_IAM_Lines/writer_IDs.pickle]

# Testing data
testing_jsons: [prepare_IAM_Lines/raw_gts/lines/txt/test.json]
testing_root: ../simple_hwr/data
testing_shuffle: false
testing_warp: false
output_predictions: true                    # Output incorrect test predictions

# Testing augmentations
testing_augmentation: true
testing_language_model: true
testing_profile_normalization: true

#Network
style_encoder: false                         # "False", "basic_encoder", "fake_encoder" - RNN + MLP (author classifier) -> embedding
batch_size: 16                               # Batch size
input_height: 60                             # Input dimension of image (will be cropped by default?)
cnn_out_size: 1024                           # Output dimension of CNN
num_of_channels: 1                           # Number of channels in input image
rnn_dimension: 512                           # originally 512

# Seq2Seq params
seq2seq: false
max_seq_len: 100
teacher_force_rate: 1.0
teacher_force_decay: 0.9
teacher_step_size: 1000
label_smoothing: true
smoothing: 0.4
cnn_load_path: ../taylor_simple_hwr/results/BEST/20190807_104745-smallv2/ONLY_THE_STATE.pt
freeze_cnn: true

# Recognizer parameters
recognizer_dropout: .5                       # Dropout in recognizer LSTM

# Writer Classifier - LSTM + MLP (with embedding layer)
writer_dropout: 0.4                          # Probability of being "dropped out" in writer LSTM+MLP
writer_rnn_dimension: 128                    # Dimension of LSTM state vector
writer_rnn_output_size: 256                  # LSTM output dimension for writer classifier
mlp_layers: [64,embedding,64]                # Layer dimensions for MLP writer classifier
embedding_size: 32                           # The size of the embedding layer in the MLP

# LR schedule
learning_rate: 1e-4          # LR
scheduler_step: 150           # Every X steps, multiply LR by gamma
scheduler_gamma: .9          # LR decay rate
