### TESTING
TESTING: false # Load a small network

# General
name: baseline
output_folder: ./results
epochs: 40
# results_dir:
# log_dir:

# Training data
training_jsons: [prepare_IAM_Lines/raw_gts/lines/txt/training.json]
training_root: "./data"
training_shuffle: true

# Testing data
testing_jsons: [prepare_IAM_Lines/raw_gts/lines/txt/test.json]
testing_root: "./data"
testing_shuffle: false

#Network
style_encoder: naive_encoder                 # "False", "naive_encoder" - RNN + MLP (author classifier) -> embedding
batch_size: 16                               # Batch size
input_height: 60                             # Input dimension of image (will be cropped by default?)
cnn_out_size: 1024                           # Output dimension of CNN
num_of_channels: 1                           # Number of channels in input image

# Recognizer parameters
recognizer_dropout: .7                       # Dropout in recognizer LSTM

# Writer Classifier - LSTM + MLP (with embedding layer)
writer_dropout: 0.4                          # Probability of being "dropped out" in writer LSTM+MLP
writer_rnn_dimension: 64                     # Dimension of LSTM state vector
writer_rnn_output_size: 64                  # LSTM output dimension for writer classifier
mlp_layers: [embedding, 128]             # Layer dimensions for MLP writer classifier
embedding_size: 32                           # The size of the embedding layer in the MLP

# LR schedule
learning_rate: 1e-3          # LR
scheduler_step: 1            # Every X steps, multiply LR by gamma
scheduler_gamma: .9          # LR decay rate

# Options
warp: false

